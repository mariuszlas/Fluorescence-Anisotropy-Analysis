<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>flu_ani_analysis.flu_ani_analysis_module API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flu_ani_analysis.flu_ani_analysis_module</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import csv
import re
import string
import math
from itertools import product
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from plate_mapping import plate_mapping as pm

# define custom errors
class DataError(Exception):
    pass

class PlateSizeError(Exception):
    pass

class DataTypeError(Exception):
    pass

# define well plate dimensions
plate_dim = {96:(8, 12), 384:(16, 24)}

# define header names
pm.header_names = {&#39;Well ID&#39;: {&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: False, &#39;short_col&#39;:False},
                &#39;Type&#39;: {&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Contents&#39;: {&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Protein Name&#39;: {&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Protein Concentration&#39;: {&#39;dtype&#39;:float, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Tracer Name&#39;: {&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Tracer Concentration&#39;: {&#39;dtype&#39;:float, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Competitor Name&#39;: {&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Competitor Concentration&#39;: {&#39;dtype&#39;:float, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                &#39;Concentration Units&#39;:{&#39;dtype&#39;:str, &#39;long&#39;:True, &#39;short_row&#39;: True, &#39;short_col&#39;:True},
                }

class FA:
    &#34;&#34;&#34;Class used for the analysis of fluorescence anisotropy data.
    
    :param data_dict: A dictionary contaning data frames with pre-processed data and metadata
    :type data_dict: dict
    :param g_factor: G-factor
    :type g_factor: float 
    :param plate_map: dataframe from a plate map csv file that defines each and every well
    :type plate_map: pandas dataframe
    &#34;&#34;&#34;
    def __init__(self, data_dict, g_factor, plate_map):
        self.data_dict = data_dict
        self.g_factor = g_factor
        self.plate_map = plate_map
        
        frames = []   # create list of all p and s data frames

        for repeat in self.data_dict.values():   
            metadata, data = repeat.values()
            p_channel, s_channel = data.values()
            frames.append(p_channel)
            frames.append(s_channel)
    
        new = pd.concat(frames, axis=1)   # join all p and s data frames into one df to run some stats
        nan = new.size - new.describe().loc[&#39;count&#39;].sum()   # find number of &#39;nan&#39; cells
        
        print(&#34;Data has been uploaded!\n&#34;)
        print(f&#34;Value of g-factor: {self.g_factor} \nNumber of repeats: {len(self.data_dict)} \nOverall number of empty cells is {int(nan)} in {len(frames)} data frames.&#34;)
              
              
    @classmethod
    def read_in_envision(cls, data_csv, platemap_csv, data_type=&#39;plate&#39;, size=384):
        &#34;&#34;&#34;Returns a dictionary of data frames, g-factor and platemap needed to construct the class object. 
        
        :param data_csv: File path of the raw data file in .csv format
        :type data_csv: str
        :param platemap_csv: File path of the platemap file in .csv format
        :type platemap_csv: str
        :param data_type: Format in which the raw data was exported (plate or list), defaults to plate
        :type data_type: str
        :param size: Size of the well plate (384 or 96), defaults to 384
        :type size: int
        :return: A dictionary contaning data frames with pre-processed data, g-factor, pandas data frame containing platemap
        :rtype: dict, float, pandas data frame &#34;&#34;&#34;
        
        # ensure the plate size is either 384 or 96
        if size not in plate_dim:
            raise PlateSizeError(&#39;Invalid size of the well plate, should be 384 or 96.&#39;)
        
        # try to read in data in plate format
        if data_type == &#39;plate&#39;:
            try:
                data_dict, g_factor = FA._read_in_plate(data_csv, size=size)
                plate_map_df = pm.plate_map(platemap_csv, size=size)
                return cls(data_dict, g_factor, plate_map_df)
            
            except (UnboundLocalError, IndexError, ValueError):
                raise DataError(f&#34;Error occured during data read in. Check your file contains data in the &#39;plate&#39; format and plate size is {size}.&#34;)
        
        # try to read in data in list format
        if data_type == &#39;list&#39;:
            try:
                data_dict, g_factor = FA._read_in_list(data_csv, size=size)
                plate_map_df = pm.plate_map(platemap_csv, size=size)
                return cls(data_dict, g_factor, plate_map_df)
            
            except (UnboundLocalError, IndexError):
                raise DataError(&#34;Error occured during data read in. Check your file contains data in the &#39;list&#39; format.&#34;)
        
        else:
            raise DataTypeError(f&#34;&#39;{data_type}&#39; is not one of the two valid data types: plate or list.&#34;)
    

                
    def _read_in_plate(csv_file, size):
        &#34;&#34;&#34;Reads the raw data file and finds the information needed to extract data. Passes those parameters to pre_process_plate function and executes it.
        Returns a tuple of two elemnts: dictionary of data frames and g-factor.

        :param csv_file: File path of the raw data file in .csv format
        :type csv_file: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A tuple of dictionary of data frames and the g-factor 
        :rtype: tuple &#34;&#34;&#34;
        
        with open(csv_file) as file:
            all_data_lines = list(csv.reader(file, delimiter=&#39;,&#39;))   # read the csv file and cast it into a list containing all lines

        blank_indexes = list(index for index, item in enumerate(all_data_lines) if item == [])   # list containing indices of all blank rows
        if blank_indexes == []:
            blank_indexes = list(index for index, item in enumerate(all_data_lines) if set(item) == {&#39;&#39;})
        blanks = np.array(blank_indexes)   # convert the list of blank indices to a numpy array
        read_in_info = []   # list to store the tuples with parameters needed for pandas to read in the csv file

        for index, item in enumerate(all_data_lines):   # iterate over all lines in the csv file
            if item != [] and re.findall(r&#34;Plate information&#34;, item[0]) == [&#39;Plate information&#39;] and re.search(r&#39;Results for&#39;, all_data_lines[index + 9][0]) == None and re.findall(r&#34;Formula&#34;, all_data_lines[index+1][10]) != [&#39;Formula&#39;]:
                skiprows = index + 9   # Set the skiprows parameter for raw data table
                skiprows_meta = index + 1   # Set the skiprows parameter for metadata table
                end_of_data = blanks[blanks &gt; skiprows].min()   # Calculate the end of data table by finding the smallest blank index after the beginning of data table
                read_in_info.append((skiprows, end_of_data - skiprows + 1, skiprows_meta))   # add the skiprows, caculated number of data lines and skiprows for metadata parameters to the list as a tuple
                data_format = &#39;plate1&#39;

            if item != [] and re.findall(r&#34;Plate information&#34;, item[0]) == [&#39;Plate information&#39;] and re.search(r&#39;Results for&#39;, all_data_lines[index + 9][0]) != None:
                skiprows = index + 10
                skiprows_meta = index + 1
                end_of_data = blanks[blanks &gt; skiprows].min()
                read_in_info.append((skiprows, end_of_data - skiprows - 1, skiprows_meta))
                data_format = &#39;plate2&#39;

            if item != [] and len(item) &gt; 1 and re.fullmatch(r&#34;G-factor&#34;, item[0]):
                g_factor = float(item[4])   
        
        return FA._pre_process_plate(csv_file, read_in_info, data_format, size), g_factor

    
    def _pre_process_plate(csv_file, read_in_info, data_format, size):    
        &#34;&#34;&#34;Extracts the data and metadata from the csv file, processes it and returns a nested dictionary containing data and metadata for each repeat and channel.

        :param csv_file: File path of the raw data file in .csv format
        :type csv_file: str
        :param read_in_info: Tuples with read in parameters for each channel.
        :type read_in_info: list
        :param data_format: Plate type (plate1 or plate2)
        :type data_format: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A dictionary containing data and metadata 
        :rtype: dict &#34;&#34;&#34; 
        
        data_frames = {}   # dictionary to store data frames
        counter = 1   # counter incremented by 0.5 to enable alternating labelling of data frames as &#39;p&#39; or &#39;s&#39;

        row_letters = list(string.ascii_uppercase)[0: plate_dim[size][0]]   # generate a list of letters for well IDs
        col_numbers = list(np.arange(1, plate_dim[size][1] + 1).astype(str))   # generate a list of numbers for well IDs
        well_ids = [&#39;%s%s&#39; % (item[0], item[1]) for item in product(row_letters, col_numbers)]   # generate a list of well IDs for the pre-processed data frames
        
        for index, item in enumerate(read_in_info):   # iterate over all tuples in the list, each tuple contains skiprows, nrows and skiprows_meta for one channel 

            if data_format == &#39;plate1&#39;:   # raw data table does not have row and column names so &#39;names&#39; parameter passed to omit the last column
                raw_data = pd.read_csv(csv_file, sep=&#39;,&#39;, names=col_numbers, index_col=False, engine=&#39;python&#39;, skiprows=item[0], nrows=item[1], encoding=&#39;utf-8&#39;)

            if data_format == &#39;plate2&#39;:   # raw data table has row and column names, so index_col=0 to set the first column as row labels
                raw_data = pd.read_csv(csv_file, sep=&#39;,&#39;, index_col=0, engine=&#39;python&#39;, skiprows=item[0], nrows=item[1], encoding=&#39;utf-8&#39;)
                if len(raw_data.columns) in [13, 25]:    
                    raw_data.drop(raw_data.columns[-1], axis=1, inplace=True)    # delete the last column because it is empty

            # generate df for metadata (number of rows of metadata table is always 1) and convert measurement time into datetime object   
            metadata = pd.read_csv(csv_file, sep=&#39;,&#39;, engine=&#39;python&#39;, skiprows=item[2], nrows=1, encoding=&#39;utf-8&#39;).astype({&#39;Measurement date&#39;: &#39;datetime64[ns]&#39;})
            # convert and reshape data frame into 1D array
            data_as_array = np.reshape(raw_data.to_numpy(), (int(size), 1)) 

            if counter % 1 == 0: 
                new_data = pd.DataFrame(data=data_as_array, index=well_ids, columns=[&#39;p&#39;])   # generate new 384 (or 96) by 1 data frame with p channel data
                data_frames[f&#39;repeat_{int(counter)}&#39;] = {&#39;metadata&#39;:metadata, &#39;data&#39;: {&#39;p&#39;: new_data, &#39;s&#39;:&#39;&#39;}}   # add p channel data and metadata dfs to dictionary

            if counter % 1 != 0:
                new_data = pd.DataFrame(data=data_as_array, index=well_ids, columns=[&#39;s&#39;])   # generate new 384 (or 96) by 1 data frame with s channel data
                data_frames[f&#39;repeat_{int(counter-0.5)}&#39;][&#39;data&#39;][&#39;s&#39;] = new_data   # add s channel data to dictionary

            counter = counter + 0.5
        
        return data_frames


    def _read_in_list(csv_file, size):
        &#34;&#34;&#34;Reads the raw data file and extracts the data and metadata. Passes the raw data to pre_process_list function and executes it.
        Returns a tuple of two elemnts: dictionary of data frames and g-factor.

        :param csv_file: File path of the raw data file in .csv format
        :type csv_file: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A tuple of dictionary of data frames and the g-factor
        :rtype: tuple &#34;&#34;&#34;

        with open(csv_file) as file:  
            all_data_lines = list(csv.reader(file, delimiter=&#39;,&#39;)) # read the csv file and cast it into a list containing all lines
 
        blank_indexes = list(index for index, item in enumerate(all_data_lines) if item == [] or set(item) == {&#39;&#39;})   # list containing indexes of all blank rows
        blanks = np.array(blank_indexes)   # convert the list of blank indexes to a numpy array
        
        # iterate over all lines to find beggining of the data table (&#39;skiprows&#39;) and determine the format of data  (list A, B, or C)
        for index, item in enumerate(all_data_lines):   
            if item != [] and len(item) == 1 and re.findall(r&#34;Plate information&#34;, item[0]) == [&#34;Plate information&#34;]:
                skiprows_meta = index + 1
                end_of_metadata = blanks[blanks &gt; skiprows_meta].min()   # find the end of metadata by finding the smallest blank index after the beginning of metadata
                
            if item != [] and len(item) &gt;= 2 and re.findall(r&#34;PlateNumber&#34;, item[0]) == [&#39;PlateNumber&#39;] and re.findall(r&#34;PlateRepeat&#34;, item[1]) == [&#39;PlateRepeat&#39;]:   # find line number with the beggining of the data
                skiprows = index - 1
                data_format = &#39;listA&#39;
                end_of_data = blanks[blanks &gt; skiprows].min()

            if item != [] and len(item) &gt;= 2 and re.findall(r&#34;Plate&#34;, item[0]) == [&#39;Plate&#39;] and re.findall(r&#34;Barcode&#34;, item[1]) == [&#39;Barcode&#39;]:   # find line number with the beggining of the data
                skiprows = index
                data_format = &#39;listB&#39;
                end_of_data = blanks[blanks &gt; skiprows].min()

            if item != [] and len(item) &gt;= 2 and re.findall(r&#34;Plate&#34;, item[0]) == [&#39;Plate&#39;]  and re.findall(r&#34;Well&#34;, item[1]) == [&#39;Well&#39;]:
                skiprows = index
                data_format = &#39;listC&#39;
                end_of_data = blanks[blanks &gt; skiprows].min()

            if item != [] and re.fullmatch(r&#34;G-factor&#34;, item[0]):   # find the g factor
                g_factor = float(item[4])

        nrows = end_of_data - skiprows - 1   # calculate the length of data table
        nrows_meta = end_of_metadata - skiprows_meta - 1   # calucalte the length of metadata table (number of rows depends on the number of repeats)

        raw_data = pd.read_csv(csv_file, sep=&#39;,&#39;, engine=&#39;python&#39;, skiprows=skiprows, nrows=nrows, encoding=&#39;utf-8&#39;)
        raw_metadata = pd.read_csv(csv_file, sep=&#39;,&#39;, engine=&#39;python&#39;, skiprows=skiprows_meta, nrows=nrows_meta, encoding=&#39;utf-8&#39;)

        return FA._pre_process_list(raw_data, raw_metadata, data_format, size), g_factor


    def _pre_process_list(raw_data, raw_metadata, data_format, size):
        &#34;&#34;&#34;Extracts the data and metadata for each channel and repeat from the raw data and raw metadata 
        and returns a nested dictionary containing data and metadata for each repeat and channel.

        :param raw_data: Data frame containing raw data
        :type raw_data: pandas data frame
        :param raw_metadata: Data frame containing raw metadata
        :type raw_metadata: pandas data frame
        :param data_format: Type of list (listA, listB, or listC)
        :type data_format: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A dictionary containing data and metadata
        :rtype: dict&#34;&#34;&#34;

        # remove the &#39;0&#39; from middle position of well numbers (A01 -&gt; A1), done by reassigning the &#39;Well&#39; column to a Series containing modified well numbers
        raw_data[&#39;Well&#39;] = raw_data[&#39;Well&#39;].apply(lambda x: x[0] + x[2] if x[1] == &#39;0&#39; else x)
        
        data_frames = {}   # dictionary to store data frames
        repeats = list(raw_metadata[&#39;Repeat&#39;].to_numpy())   # generate a list with repeats based on the metadata table, e.g. for 3 repeats -&gt; [1,2,3]

        row_letters = list(string.ascii_uppercase)[0: plate_dim[size][0]]   # generate a list of letters for well IDs
        col_numbers = list(np.arange(1, plate_dim[size][1] + 1).astype(str))   # generate a list of numbers for well IDs
        well_ids = [&#39;%s%s&#39; % (item[0], item[1]) for item in product(row_letters, col_numbers)]   # generate a list of well IDs for the pre-processed data frames
        
        for index, repeat in enumerate(repeats):   # iterate over the number of repeats
            if data_format == &#39;listA&#39;:
                groupped_data = raw_data.groupby(raw_data.PlateRepeat).get_group(repeat)   # group and extract the data by the plate repeat column, i.e. in each iteration get data only for the current repeat 

                p_groupped = groupped_data.iloc[::3, :]   # extract data only for the p channel, i.e. each third row starting from the first row
                s_groupped = groupped_data.iloc[1::3, :]   # extract data only for the s channel, i.e. each third row starting from the second row

                p_raw_data = p_groupped[[&#39;Well&#39;, &#39;Signal&#39;]]   # extract only the two relevant columns
                s_raw_data = s_groupped[[&#39;Well&#39;, &#39;Signal&#39;]]   # for each channel

            if data_format in [&#39;listB&#39;, &#39;listC&#39;]: 
                # the column naming is different for the first repeat (&#39;Signal&#39;), then it&#39;s &#39;Signal.1&#39;, &#39;Signal.2&#39;, etc.
                if repeat == 1: 
                    p_raw_data = raw_data[[&#39;Well&#39;, &#39;Signal&#39;]]   
                    s_raw_data = raw_data[[&#39;Well&#39;, f&#39;Signal.{repeat}&#39;]]
                else:
                    p_raw_data = raw_data[[&#39;Well&#39;, f&#39;Signal.{repeat + index - 1}&#39;]]   # the column cotntaining data to be extracted is calculated in each iteration
                    s_raw_data = raw_data[[&#39;Well&#39;, f&#39;Signal.{repeat + index}&#39;]]
            
            # create an empty df with no columns and indexes matching the plate size
            indexes = pd.DataFrame(well_ids, columns=[&#39;Wells&#39;])
            empty_frame = indexes.set_index(&#39;Wells&#39;)
            
            p_raw_data.set_index(&#39;Well&#39;, inplace=True)   # set the row indexes as the well numbers
            p_raw_data.set_axis([&#39;p&#39;], axis=1, inplace=True)   # rename the &#39;Signal&#39; column to &#39;p&#39;
            p_data = empty_frame.join(p_raw_data)   # join the raw data df to an empty frame based on the indexes, assigns &#39;NaN&#39; to indexes not present in the raw data table
            
            s_raw_data.set_index(&#39;Well&#39;, inplace=True) 
            s_raw_data.set_axis([&#39;s&#39;], axis=1, inplace=True)
            s_data = empty_frame.join(s_raw_data)
    
            metadata = raw_metadata.iloc[[repeat-1]].astype({&#39;Measurement date&#39;: &#39;datetime64[ns]&#39;})   # extract the row with metadata relevant for each repeat and covert date and time into a datetime object
            data_frames[f&#39;repeat_{repeat}&#39;] = {&#39;metadata&#39;: metadata, &#39;data&#39;: {&#39;p&#39;: p_data, &#39;s&#39;: s_data}}   # add data frames to the dictionary

        return data_frames
    
    
    def visualise(self, colorby=&#39;Type&#39;, labelby=&#39;Type&#39;, title=&#34;&#34;, cmap=&#39;Paired&#39;, dpi=250, export=False):
        &#34;&#34;&#34;Returns a visual representation of the plate map.
        The label and colour for each well can be customised to be a variable, for example &#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;, etc.
        It can also be the p or s anisotropy value from a specified repeat passed as a tuple of strings, for example (&#39;repeat_2&#39;, &#39;p&#39;) for p data from repeat 2
        
        :param colorby: Chooses the parameter to color code by, for example &#39;Type&#39;, &#39;Contents&#39;, &#39;Protein Concentration&#39;, (&#39;repeat_2&#39;, &#39;p&#39;), default = &#39;Type&#39;
        :type colorby: str or tuple
        :param labelby: Chooses the parameter to label code by, for example &#39;Type&#39;, &#39;Contents&#39;, &#39;Protein&#39;, (&#39;repeat_1&#39;, &#39;s&#39;), default = &#39;Type&#39;
        :type labelby: str or tuple
        :param title: Sets the title of the figure, default none
        :type title: str
        :param cmap: Sets the colormap for the color-coding, default = &#39;Paired&#39;
        :type cmap: str
        :param dpi: Size of the figure, default = 250
        :type dpi: int
        :param export: If &#39;True&#39; a .png file of the figure is saved, default = False
        :type export: bool
        :return: Visual representation of the plate map.
        :rtype: figure
        &#34;&#34;&#34;
        plate_map = self.plate_map
        scinot = False
        
        if type(labelby) == tuple:   # option for labelling by the p or s anisotropy values
            plate_map = self.plate_map.join(self.data_dict[labelby[0]][&#39;data&#39;][labelby[1]])   # data frame containing p or s values from specified repeat is added to the platemap
            labelby = labelby[1]
        if type(colorby) == tuple:   # option for colouring by the p or s anisotropy values
            plate_map = self.plate_map.join(self.data_dict[colorby[0]][&#39;data&#39;][colorby[1]])
            colorby = colorby[1]
            
        size = plate_map.shape[0]
        
        if labelby in [&#39;Protein Concentration&#39;, &#39;Tracer Concentration&#39;, &#39;Competitor Concentration&#39;, &#39;p&#39;, &#39;s&#39;, &#39;p_corrected&#39;, &#39;s_corrected&#39;, &#39;r_raw&#39;, &#39;r_corrected&#39;, &#39;i_raw&#39; , &#39;i_corrected&#39;]:
            if sum((plate_map[labelby] &gt; 1000) | (plate_map[labelby] &lt; 0)) &gt; 0:   # display in sci notation if the number is greater than 1000 or less than 0
                scinot = True
        
        return pm.visualise(platemap=plate_map, title=title, size=size, export=export, cmap=cmap, colorby=colorby, labelby=labelby, scinot=scinot, dpi=dpi)
    
    
    def invalidate(self, valid=False, **kwargs):
        &#34;&#34;&#34;Invalidates wells, columns and/or rows. Any of the following arguments, or their combination, can be passed: wells, rows, columns. 
        For example to invalidate well A1, rows C and D and columns 7 and 8 execute the following: invalidate(wells=&#39;A1&#39;, rows=[&#39;C&#39;,&#39;D&#39;], columns=[7,8]).
        To validate previously invalidated wells, rows and/or columns, pass the additional &#39;valid&#39; argument as True.
    
        :param valid: Sets the stipulated row or rows &#39;True&#39; or &#39;False&#39;, default = False
        :type valid: bool
        :param wells: Wells to be invalidated passed as a string or a list of strings
        :type wells: str or list
        :param rows: Rows to be invalidated passed as a string or a list of strings
        :type rows: str or list
        :param columns: Columns to be invalidated passed as an integer or a list of integers
        :type columns: int or list
        &#34;&#34;&#34;
        # execute the corresponding invalidate functon from the platemapping package
        if &#39;wells&#39; in kwargs:
            pm.invalidate_wells(platemap=self.plate_map, wells=kwargs[&#39;wells&#39;], valid=valid)
        if &#39;rows&#39; in kwargs:
            rows = tuple(kwargs[&#39;rows&#39;]) # convert the rows to tuple because invalidate_rows cannot take in a list
            pm.invalidate_rows(platemap=self.plate_map, rows=rows, valid=valid)
        if &#39;columns&#39; in kwargs:
            pm.invalidate_cols(platemap=self.plate_map, cols=kwargs[&#39;columns&#39;], valid=valid)
        if len(kwargs) == 0:   # return error if neither of the keyword arguments is passed
            raise TypeError(&#39;No arguments were passed. Specify the wells, rows or columns to be invalidated!&#39;)
            
            
    def background_correct(self):
        &#34;&#34;&#34;Calculate background corrected values for p and s channel in all repeats.
        
        Cacluclated by subtracting the mean value of blank p or s for a given concentration from each value of compound p or s for that concentration.&#34;&#34;&#34;
        
        for key, value in self.data_dict.items(): 
            metadata, data = value.values()   
            p, s = data.values()             
            
            # create joined dfs of platemap and p or s
            p_df = self.plate_map.join(p)  
            s_df = self.plate_map.join(s)
            
            # calculate p and s corrected and add them to data dictionary
            self.data_dict[key][&#39;data&#39;][&#39;p_corrected&#39;] = FA._backg_correct(p_df, &#39;p_corrected&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;s_corrected&#39;] = FA._backg_correct(s_df, &#39;s_corrected&#39;)
            
            print(&#39;Background correction has been successfully performed!&#39;)
            
    def _backg_correct(df, col_name):
        &#34;&#34;&#34;Calculate background corrected p or s.
        
        :param df: Joined platemap and raw p or s values 
        :type df: pandas df
        :param col_name: Name of the column with background corrected values
        :type col_name: str
        :return: Data frame with background corrected p or s values (depending on col_name parameter)
        :rtype: pandas df&#34;&#34;&#34;
        
        df[df.columns[-1]] = df[df.columns[-1]][df[&#39;Valid&#39;] == True]   # &#39;p&#39; or &#39;s&#39; values are replaced with NaN if the well is invalidated
        no_index = df.reset_index()   # move the index to df column

        mindex = pd.MultiIndex.from_frame(no_index[[&#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;]])   # create multiindex
        reindexed = no_index.set_index(mindex).drop([&#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;], axis=1)   # add multiindex to df and drop the columns from which multiindex was created
        
        mean = reindexed.groupby(level=[0,1,2]).mean().drop(&#39;Valid&#39;, axis=1)   # calculate mean for each group of three wells and remove &#39;Valid&#39; column
        mean.rename(columns={mean.columns[-1]: &#39;Mean&#39;}, inplace=True)   # rename the last column to &#39;Mean&#39;

        blank = mean.xs(&#39;blank&#39;, level=0, drop_level=True)   # take a group with blank wells
        
        joined = reindexed.join(blank, on=[&#39;Protein Name&#39;, &#39;Protein Concentration&#39;])
        joined[col_name] = joined[joined.columns[-2]] - joined[&#39;Mean&#39;]   # calculate background corrected values
        jindexed = joined.set_index(&#39;index&#39;, append=True).reset_index(level=[0,1,2]).rename_axis(None)   # set index to &#39;well id&#39; and move multiindex to df columns
        
        return jindexed[[col_name]]
    
    
    def calculate_r_i(self, corrected=True, th=80):
        &#34;&#34;&#34;Calculates anisotropy and fluorescence intensity.
        The fluorescence intensity (I) and anisotropy (r) are calculated using the follwing formulas: I = s + (2*g*p), r = (s - (g*p)) / I and stored 
        in data_dict as i_raw and r_raw (calculated using the uncorrected p and s channel values) 
        and i_corrected and r_corrected (if calculated using the background corrected p and s channel values, as well).
        
        :param corrected: Calculate the anisotropy and intensity using the background corrected values of p and s, as well, default=True
        :type corrected: bool&#34;&#34;&#34;
        d = []
        
        for key, value in self.data_dict.items():   # iterate over all repeats
            metadata, data = value.values()
            
            # calculate raw intensity and anisotropy and add them to data dictionary
            i, r = FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;i_raw&#39;] = i   #FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;, &#39;i&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;r_raw&#39;] = r   #FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;, &#39;r&#39;)
            
            if corrected:   # calculate intensity and anisotropy using background corrected values of p and s
                if &#39;p_corrected&#39; and &#39;s_corrected&#39; not in data:   # check if background subtraction has been done
                    raise AttributeError(&#39;The corrected anisotropy and intensity can only be calculated after background correction of the raw p and s channel data.&#39;)
                
                i_c, r_c = FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;)
                self.data_dict[key][&#39;data&#39;][&#39;i_corrected&#39;] = i_c   #FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;, &#39;i&#39;)
                self.data_dict[key][&#39;data&#39;][&#39;r_corrected&#39;] = r_c   #FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;, &#39;r&#39;) 
                
                d.append(FA._calc_I_percent(key, th, i, i_c, self.plate_map))
                
        print(&#39;Fluorescence intensity and anisotropy have been calculated!\n&#39;)
        print(d)

        
    def _calc_r_I(p, s, g, col_suffix):
        &#34;&#34;&#34;Calculates either anisotropy or intensity and labels the resulting dfs according to the parameters passed
        
        :param p: Pandas data frame with p channel data (can be both raw and background corrected)
        :type p: pandas df 
        :param s: Pandas data frame with s channel data (can be both raw and background corrected)
        :type s: pandas df
        :param col_suffix: Suffix to add to column name of the resulting intensity or anisotropy data frame, e.g. &#39;raw&#39;, &#39;corrected&#39;
        :type col_suffix: str
        :return: Data frame with calculated anisotropy or intensity values
        :rtype: pandas df&#34;&#34;&#34;
        
        p_rn = p.rename(columns={p.columns[0]: s.columns[0]})   # rename the col name in p data frame so that both p and s dfs have the same col names to enable calculation on dfs
        i = s + (2 * g * p_rn)   # calculate intensity irrespectively of the var arguent    
        r = (s - (g * p_rn)) / i
        i_rn = i.rename(columns={i.columns[0]: &#39;i_&#39;+col_suffix})
        r_rn = r.rename(columns={r.columns[0]: &#39;r_&#39;+col_suffix})   # rename the col name        
            
        return i_rn, r_rn  
    
    
    def _calc_I_percent(rep, th, ir, ic, platemap):
        ir_rn = ir.rename(columns={ir.columns[0]:ic.columns[0]})
        ratio = (ir_rn - ic)/ir_rn * 100
        ratio.rename(columns={&#39;i_corrected&#39;:&#39;percent&#39;}, inplace=True)
        j = platemap.join(ratio)
        subset = j[(j[&#39;Type&#39;] != &#39;blank&#39;) &amp; (j[&#39;Type&#39;] != &#39;empty&#39;)]
        wells = list(subset[subset[&#39;percent&#39;] &gt; th].index)
        return rep, wells, subset</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flu_ani_analysis.flu_ani_analysis_module.DataError"><code class="flex name class">
<span>class <span class="ident">DataError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataError(Exception):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="flu_ani_analysis.flu_ani_analysis_module.DataTypeError"><code class="flex name class">
<span>class <span class="ident">DataTypeError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataTypeError(Exception):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="flu_ani_analysis.flu_ani_analysis_module.FA"><code class="flex name class">
<span>class <span class="ident">FA</span></span>
<span>(</span><span>data_dict, g_factor, plate_map)</span>
</code></dt>
<dd>
<div class="desc"><p>Class used for the analysis of fluorescence anisotropy data.</p>
<p>:param data_dict: A dictionary contaning data frames with pre-processed data and metadata
:type data_dict: dict
:param g_factor: G-factor
:type g_factor: float
:param plate_map: dataframe from a plate map csv file that defines each and every well
:type plate_map: pandas dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FA:
    &#34;&#34;&#34;Class used for the analysis of fluorescence anisotropy data.
    
    :param data_dict: A dictionary contaning data frames with pre-processed data and metadata
    :type data_dict: dict
    :param g_factor: G-factor
    :type g_factor: float 
    :param plate_map: dataframe from a plate map csv file that defines each and every well
    :type plate_map: pandas dataframe
    &#34;&#34;&#34;
    def __init__(self, data_dict, g_factor, plate_map):
        self.data_dict = data_dict
        self.g_factor = g_factor
        self.plate_map = plate_map
        
        frames = []   # create list of all p and s data frames

        for repeat in self.data_dict.values():   
            metadata, data = repeat.values()
            p_channel, s_channel = data.values()
            frames.append(p_channel)
            frames.append(s_channel)
    
        new = pd.concat(frames, axis=1)   # join all p and s data frames into one df to run some stats
        nan = new.size - new.describe().loc[&#39;count&#39;].sum()   # find number of &#39;nan&#39; cells
        
        print(&#34;Data has been uploaded!\n&#34;)
        print(f&#34;Value of g-factor: {self.g_factor} \nNumber of repeats: {len(self.data_dict)} \nOverall number of empty cells is {int(nan)} in {len(frames)} data frames.&#34;)
              
              
    @classmethod
    def read_in_envision(cls, data_csv, platemap_csv, data_type=&#39;plate&#39;, size=384):
        &#34;&#34;&#34;Returns a dictionary of data frames, g-factor and platemap needed to construct the class object. 
        
        :param data_csv: File path of the raw data file in .csv format
        :type data_csv: str
        :param platemap_csv: File path of the platemap file in .csv format
        :type platemap_csv: str
        :param data_type: Format in which the raw data was exported (plate or list), defaults to plate
        :type data_type: str
        :param size: Size of the well plate (384 or 96), defaults to 384
        :type size: int
        :return: A dictionary contaning data frames with pre-processed data, g-factor, pandas data frame containing platemap
        :rtype: dict, float, pandas data frame &#34;&#34;&#34;
        
        # ensure the plate size is either 384 or 96
        if size not in plate_dim:
            raise PlateSizeError(&#39;Invalid size of the well plate, should be 384 or 96.&#39;)
        
        # try to read in data in plate format
        if data_type == &#39;plate&#39;:
            try:
                data_dict, g_factor = FA._read_in_plate(data_csv, size=size)
                plate_map_df = pm.plate_map(platemap_csv, size=size)
                return cls(data_dict, g_factor, plate_map_df)
            
            except (UnboundLocalError, IndexError, ValueError):
                raise DataError(f&#34;Error occured during data read in. Check your file contains data in the &#39;plate&#39; format and plate size is {size}.&#34;)
        
        # try to read in data in list format
        if data_type == &#39;list&#39;:
            try:
                data_dict, g_factor = FA._read_in_list(data_csv, size=size)
                plate_map_df = pm.plate_map(platemap_csv, size=size)
                return cls(data_dict, g_factor, plate_map_df)
            
            except (UnboundLocalError, IndexError):
                raise DataError(&#34;Error occured during data read in. Check your file contains data in the &#39;list&#39; format.&#34;)
        
        else:
            raise DataTypeError(f&#34;&#39;{data_type}&#39; is not one of the two valid data types: plate or list.&#34;)
    

                
    def _read_in_plate(csv_file, size):
        &#34;&#34;&#34;Reads the raw data file and finds the information needed to extract data. Passes those parameters to pre_process_plate function and executes it.
        Returns a tuple of two elemnts: dictionary of data frames and g-factor.

        :param csv_file: File path of the raw data file in .csv format
        :type csv_file: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A tuple of dictionary of data frames and the g-factor 
        :rtype: tuple &#34;&#34;&#34;
        
        with open(csv_file) as file:
            all_data_lines = list(csv.reader(file, delimiter=&#39;,&#39;))   # read the csv file and cast it into a list containing all lines

        blank_indexes = list(index for index, item in enumerate(all_data_lines) if item == [])   # list containing indices of all blank rows
        if blank_indexes == []:
            blank_indexes = list(index for index, item in enumerate(all_data_lines) if set(item) == {&#39;&#39;})
        blanks = np.array(blank_indexes)   # convert the list of blank indices to a numpy array
        read_in_info = []   # list to store the tuples with parameters needed for pandas to read in the csv file

        for index, item in enumerate(all_data_lines):   # iterate over all lines in the csv file
            if item != [] and re.findall(r&#34;Plate information&#34;, item[0]) == [&#39;Plate information&#39;] and re.search(r&#39;Results for&#39;, all_data_lines[index + 9][0]) == None and re.findall(r&#34;Formula&#34;, all_data_lines[index+1][10]) != [&#39;Formula&#39;]:
                skiprows = index + 9   # Set the skiprows parameter for raw data table
                skiprows_meta = index + 1   # Set the skiprows parameter for metadata table
                end_of_data = blanks[blanks &gt; skiprows].min()   # Calculate the end of data table by finding the smallest blank index after the beginning of data table
                read_in_info.append((skiprows, end_of_data - skiprows + 1, skiprows_meta))   # add the skiprows, caculated number of data lines and skiprows for metadata parameters to the list as a tuple
                data_format = &#39;plate1&#39;

            if item != [] and re.findall(r&#34;Plate information&#34;, item[0]) == [&#39;Plate information&#39;] and re.search(r&#39;Results for&#39;, all_data_lines[index + 9][0]) != None:
                skiprows = index + 10
                skiprows_meta = index + 1
                end_of_data = blanks[blanks &gt; skiprows].min()
                read_in_info.append((skiprows, end_of_data - skiprows - 1, skiprows_meta))
                data_format = &#39;plate2&#39;

            if item != [] and len(item) &gt; 1 and re.fullmatch(r&#34;G-factor&#34;, item[0]):
                g_factor = float(item[4])   
        
        return FA._pre_process_plate(csv_file, read_in_info, data_format, size), g_factor

    
    def _pre_process_plate(csv_file, read_in_info, data_format, size):    
        &#34;&#34;&#34;Extracts the data and metadata from the csv file, processes it and returns a nested dictionary containing data and metadata for each repeat and channel.

        :param csv_file: File path of the raw data file in .csv format
        :type csv_file: str
        :param read_in_info: Tuples with read in parameters for each channel.
        :type read_in_info: list
        :param data_format: Plate type (plate1 or plate2)
        :type data_format: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A dictionary containing data and metadata 
        :rtype: dict &#34;&#34;&#34; 
        
        data_frames = {}   # dictionary to store data frames
        counter = 1   # counter incremented by 0.5 to enable alternating labelling of data frames as &#39;p&#39; or &#39;s&#39;

        row_letters = list(string.ascii_uppercase)[0: plate_dim[size][0]]   # generate a list of letters for well IDs
        col_numbers = list(np.arange(1, plate_dim[size][1] + 1).astype(str))   # generate a list of numbers for well IDs
        well_ids = [&#39;%s%s&#39; % (item[0], item[1]) for item in product(row_letters, col_numbers)]   # generate a list of well IDs for the pre-processed data frames
        
        for index, item in enumerate(read_in_info):   # iterate over all tuples in the list, each tuple contains skiprows, nrows and skiprows_meta for one channel 

            if data_format == &#39;plate1&#39;:   # raw data table does not have row and column names so &#39;names&#39; parameter passed to omit the last column
                raw_data = pd.read_csv(csv_file, sep=&#39;,&#39;, names=col_numbers, index_col=False, engine=&#39;python&#39;, skiprows=item[0], nrows=item[1], encoding=&#39;utf-8&#39;)

            if data_format == &#39;plate2&#39;:   # raw data table has row and column names, so index_col=0 to set the first column as row labels
                raw_data = pd.read_csv(csv_file, sep=&#39;,&#39;, index_col=0, engine=&#39;python&#39;, skiprows=item[0], nrows=item[1], encoding=&#39;utf-8&#39;)
                if len(raw_data.columns) in [13, 25]:    
                    raw_data.drop(raw_data.columns[-1], axis=1, inplace=True)    # delete the last column because it is empty

            # generate df for metadata (number of rows of metadata table is always 1) and convert measurement time into datetime object   
            metadata = pd.read_csv(csv_file, sep=&#39;,&#39;, engine=&#39;python&#39;, skiprows=item[2], nrows=1, encoding=&#39;utf-8&#39;).astype({&#39;Measurement date&#39;: &#39;datetime64[ns]&#39;})
            # convert and reshape data frame into 1D array
            data_as_array = np.reshape(raw_data.to_numpy(), (int(size), 1)) 

            if counter % 1 == 0: 
                new_data = pd.DataFrame(data=data_as_array, index=well_ids, columns=[&#39;p&#39;])   # generate new 384 (or 96) by 1 data frame with p channel data
                data_frames[f&#39;repeat_{int(counter)}&#39;] = {&#39;metadata&#39;:metadata, &#39;data&#39;: {&#39;p&#39;: new_data, &#39;s&#39;:&#39;&#39;}}   # add p channel data and metadata dfs to dictionary

            if counter % 1 != 0:
                new_data = pd.DataFrame(data=data_as_array, index=well_ids, columns=[&#39;s&#39;])   # generate new 384 (or 96) by 1 data frame with s channel data
                data_frames[f&#39;repeat_{int(counter-0.5)}&#39;][&#39;data&#39;][&#39;s&#39;] = new_data   # add s channel data to dictionary

            counter = counter + 0.5
        
        return data_frames


    def _read_in_list(csv_file, size):
        &#34;&#34;&#34;Reads the raw data file and extracts the data and metadata. Passes the raw data to pre_process_list function and executes it.
        Returns a tuple of two elemnts: dictionary of data frames and g-factor.

        :param csv_file: File path of the raw data file in .csv format
        :type csv_file: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A tuple of dictionary of data frames and the g-factor
        :rtype: tuple &#34;&#34;&#34;

        with open(csv_file) as file:  
            all_data_lines = list(csv.reader(file, delimiter=&#39;,&#39;)) # read the csv file and cast it into a list containing all lines
 
        blank_indexes = list(index for index, item in enumerate(all_data_lines) if item == [] or set(item) == {&#39;&#39;})   # list containing indexes of all blank rows
        blanks = np.array(blank_indexes)   # convert the list of blank indexes to a numpy array
        
        # iterate over all lines to find beggining of the data table (&#39;skiprows&#39;) and determine the format of data  (list A, B, or C)
        for index, item in enumerate(all_data_lines):   
            if item != [] and len(item) == 1 and re.findall(r&#34;Plate information&#34;, item[0]) == [&#34;Plate information&#34;]:
                skiprows_meta = index + 1
                end_of_metadata = blanks[blanks &gt; skiprows_meta].min()   # find the end of metadata by finding the smallest blank index after the beginning of metadata
                
            if item != [] and len(item) &gt;= 2 and re.findall(r&#34;PlateNumber&#34;, item[0]) == [&#39;PlateNumber&#39;] and re.findall(r&#34;PlateRepeat&#34;, item[1]) == [&#39;PlateRepeat&#39;]:   # find line number with the beggining of the data
                skiprows = index - 1
                data_format = &#39;listA&#39;
                end_of_data = blanks[blanks &gt; skiprows].min()

            if item != [] and len(item) &gt;= 2 and re.findall(r&#34;Plate&#34;, item[0]) == [&#39;Plate&#39;] and re.findall(r&#34;Barcode&#34;, item[1]) == [&#39;Barcode&#39;]:   # find line number with the beggining of the data
                skiprows = index
                data_format = &#39;listB&#39;
                end_of_data = blanks[blanks &gt; skiprows].min()

            if item != [] and len(item) &gt;= 2 and re.findall(r&#34;Plate&#34;, item[0]) == [&#39;Plate&#39;]  and re.findall(r&#34;Well&#34;, item[1]) == [&#39;Well&#39;]:
                skiprows = index
                data_format = &#39;listC&#39;
                end_of_data = blanks[blanks &gt; skiprows].min()

            if item != [] and re.fullmatch(r&#34;G-factor&#34;, item[0]):   # find the g factor
                g_factor = float(item[4])

        nrows = end_of_data - skiprows - 1   # calculate the length of data table
        nrows_meta = end_of_metadata - skiprows_meta - 1   # calucalte the length of metadata table (number of rows depends on the number of repeats)

        raw_data = pd.read_csv(csv_file, sep=&#39;,&#39;, engine=&#39;python&#39;, skiprows=skiprows, nrows=nrows, encoding=&#39;utf-8&#39;)
        raw_metadata = pd.read_csv(csv_file, sep=&#39;,&#39;, engine=&#39;python&#39;, skiprows=skiprows_meta, nrows=nrows_meta, encoding=&#39;utf-8&#39;)

        return FA._pre_process_list(raw_data, raw_metadata, data_format, size), g_factor


    def _pre_process_list(raw_data, raw_metadata, data_format, size):
        &#34;&#34;&#34;Extracts the data and metadata for each channel and repeat from the raw data and raw metadata 
        and returns a nested dictionary containing data and metadata for each repeat and channel.

        :param raw_data: Data frame containing raw data
        :type raw_data: pandas data frame
        :param raw_metadata: Data frame containing raw metadata
        :type raw_metadata: pandas data frame
        :param data_format: Type of list (listA, listB, or listC)
        :type data_format: str
        :param well_ids: A list of well IDs for the pre-processed data frames
        :type well_ids: list
        :return: A dictionary containing data and metadata
        :rtype: dict&#34;&#34;&#34;

        # remove the &#39;0&#39; from middle position of well numbers (A01 -&gt; A1), done by reassigning the &#39;Well&#39; column to a Series containing modified well numbers
        raw_data[&#39;Well&#39;] = raw_data[&#39;Well&#39;].apply(lambda x: x[0] + x[2] if x[1] == &#39;0&#39; else x)
        
        data_frames = {}   # dictionary to store data frames
        repeats = list(raw_metadata[&#39;Repeat&#39;].to_numpy())   # generate a list with repeats based on the metadata table, e.g. for 3 repeats -&gt; [1,2,3]

        row_letters = list(string.ascii_uppercase)[0: plate_dim[size][0]]   # generate a list of letters for well IDs
        col_numbers = list(np.arange(1, plate_dim[size][1] + 1).astype(str))   # generate a list of numbers for well IDs
        well_ids = [&#39;%s%s&#39; % (item[0], item[1]) for item in product(row_letters, col_numbers)]   # generate a list of well IDs for the pre-processed data frames
        
        for index, repeat in enumerate(repeats):   # iterate over the number of repeats
            if data_format == &#39;listA&#39;:
                groupped_data = raw_data.groupby(raw_data.PlateRepeat).get_group(repeat)   # group and extract the data by the plate repeat column, i.e. in each iteration get data only for the current repeat 

                p_groupped = groupped_data.iloc[::3, :]   # extract data only for the p channel, i.e. each third row starting from the first row
                s_groupped = groupped_data.iloc[1::3, :]   # extract data only for the s channel, i.e. each third row starting from the second row

                p_raw_data = p_groupped[[&#39;Well&#39;, &#39;Signal&#39;]]   # extract only the two relevant columns
                s_raw_data = s_groupped[[&#39;Well&#39;, &#39;Signal&#39;]]   # for each channel

            if data_format in [&#39;listB&#39;, &#39;listC&#39;]: 
                # the column naming is different for the first repeat (&#39;Signal&#39;), then it&#39;s &#39;Signal.1&#39;, &#39;Signal.2&#39;, etc.
                if repeat == 1: 
                    p_raw_data = raw_data[[&#39;Well&#39;, &#39;Signal&#39;]]   
                    s_raw_data = raw_data[[&#39;Well&#39;, f&#39;Signal.{repeat}&#39;]]
                else:
                    p_raw_data = raw_data[[&#39;Well&#39;, f&#39;Signal.{repeat + index - 1}&#39;]]   # the column cotntaining data to be extracted is calculated in each iteration
                    s_raw_data = raw_data[[&#39;Well&#39;, f&#39;Signal.{repeat + index}&#39;]]
            
            # create an empty df with no columns and indexes matching the plate size
            indexes = pd.DataFrame(well_ids, columns=[&#39;Wells&#39;])
            empty_frame = indexes.set_index(&#39;Wells&#39;)
            
            p_raw_data.set_index(&#39;Well&#39;, inplace=True)   # set the row indexes as the well numbers
            p_raw_data.set_axis([&#39;p&#39;], axis=1, inplace=True)   # rename the &#39;Signal&#39; column to &#39;p&#39;
            p_data = empty_frame.join(p_raw_data)   # join the raw data df to an empty frame based on the indexes, assigns &#39;NaN&#39; to indexes not present in the raw data table
            
            s_raw_data.set_index(&#39;Well&#39;, inplace=True) 
            s_raw_data.set_axis([&#39;s&#39;], axis=1, inplace=True)
            s_data = empty_frame.join(s_raw_data)
    
            metadata = raw_metadata.iloc[[repeat-1]].astype({&#39;Measurement date&#39;: &#39;datetime64[ns]&#39;})   # extract the row with metadata relevant for each repeat and covert date and time into a datetime object
            data_frames[f&#39;repeat_{repeat}&#39;] = {&#39;metadata&#39;: metadata, &#39;data&#39;: {&#39;p&#39;: p_data, &#39;s&#39;: s_data}}   # add data frames to the dictionary

        return data_frames
    
    
    def visualise(self, colorby=&#39;Type&#39;, labelby=&#39;Type&#39;, title=&#34;&#34;, cmap=&#39;Paired&#39;, dpi=250, export=False):
        &#34;&#34;&#34;Returns a visual representation of the plate map.
        The label and colour for each well can be customised to be a variable, for example &#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;, etc.
        It can also be the p or s anisotropy value from a specified repeat passed as a tuple of strings, for example (&#39;repeat_2&#39;, &#39;p&#39;) for p data from repeat 2
        
        :param colorby: Chooses the parameter to color code by, for example &#39;Type&#39;, &#39;Contents&#39;, &#39;Protein Concentration&#39;, (&#39;repeat_2&#39;, &#39;p&#39;), default = &#39;Type&#39;
        :type colorby: str or tuple
        :param labelby: Chooses the parameter to label code by, for example &#39;Type&#39;, &#39;Contents&#39;, &#39;Protein&#39;, (&#39;repeat_1&#39;, &#39;s&#39;), default = &#39;Type&#39;
        :type labelby: str or tuple
        :param title: Sets the title of the figure, default none
        :type title: str
        :param cmap: Sets the colormap for the color-coding, default = &#39;Paired&#39;
        :type cmap: str
        :param dpi: Size of the figure, default = 250
        :type dpi: int
        :param export: If &#39;True&#39; a .png file of the figure is saved, default = False
        :type export: bool
        :return: Visual representation of the plate map.
        :rtype: figure
        &#34;&#34;&#34;
        plate_map = self.plate_map
        scinot = False
        
        if type(labelby) == tuple:   # option for labelling by the p or s anisotropy values
            plate_map = self.plate_map.join(self.data_dict[labelby[0]][&#39;data&#39;][labelby[1]])   # data frame containing p or s values from specified repeat is added to the platemap
            labelby = labelby[1]
        if type(colorby) == tuple:   # option for colouring by the p or s anisotropy values
            plate_map = self.plate_map.join(self.data_dict[colorby[0]][&#39;data&#39;][colorby[1]])
            colorby = colorby[1]
            
        size = plate_map.shape[0]
        
        if labelby in [&#39;Protein Concentration&#39;, &#39;Tracer Concentration&#39;, &#39;Competitor Concentration&#39;, &#39;p&#39;, &#39;s&#39;, &#39;p_corrected&#39;, &#39;s_corrected&#39;, &#39;r_raw&#39;, &#39;r_corrected&#39;, &#39;i_raw&#39; , &#39;i_corrected&#39;]:
            if sum((plate_map[labelby] &gt; 1000) | (plate_map[labelby] &lt; 0)) &gt; 0:   # display in sci notation if the number is greater than 1000 or less than 0
                scinot = True
        
        return pm.visualise(platemap=plate_map, title=title, size=size, export=export, cmap=cmap, colorby=colorby, labelby=labelby, scinot=scinot, dpi=dpi)
    
    
    def invalidate(self, valid=False, **kwargs):
        &#34;&#34;&#34;Invalidates wells, columns and/or rows. Any of the following arguments, or their combination, can be passed: wells, rows, columns. 
        For example to invalidate well A1, rows C and D and columns 7 and 8 execute the following: invalidate(wells=&#39;A1&#39;, rows=[&#39;C&#39;,&#39;D&#39;], columns=[7,8]).
        To validate previously invalidated wells, rows and/or columns, pass the additional &#39;valid&#39; argument as True.
    
        :param valid: Sets the stipulated row or rows &#39;True&#39; or &#39;False&#39;, default = False
        :type valid: bool
        :param wells: Wells to be invalidated passed as a string or a list of strings
        :type wells: str or list
        :param rows: Rows to be invalidated passed as a string or a list of strings
        :type rows: str or list
        :param columns: Columns to be invalidated passed as an integer or a list of integers
        :type columns: int or list
        &#34;&#34;&#34;
        # execute the corresponding invalidate functon from the platemapping package
        if &#39;wells&#39; in kwargs:
            pm.invalidate_wells(platemap=self.plate_map, wells=kwargs[&#39;wells&#39;], valid=valid)
        if &#39;rows&#39; in kwargs:
            rows = tuple(kwargs[&#39;rows&#39;]) # convert the rows to tuple because invalidate_rows cannot take in a list
            pm.invalidate_rows(platemap=self.plate_map, rows=rows, valid=valid)
        if &#39;columns&#39; in kwargs:
            pm.invalidate_cols(platemap=self.plate_map, cols=kwargs[&#39;columns&#39;], valid=valid)
        if len(kwargs) == 0:   # return error if neither of the keyword arguments is passed
            raise TypeError(&#39;No arguments were passed. Specify the wells, rows or columns to be invalidated!&#39;)
            
            
    def background_correct(self):
        &#34;&#34;&#34;Calculate background corrected values for p and s channel in all repeats.
        
        Cacluclated by subtracting the mean value of blank p or s for a given concentration from each value of compound p or s for that concentration.&#34;&#34;&#34;
        
        for key, value in self.data_dict.items(): 
            metadata, data = value.values()   
            p, s = data.values()             
            
            # create joined dfs of platemap and p or s
            p_df = self.plate_map.join(p)  
            s_df = self.plate_map.join(s)
            
            # calculate p and s corrected and add them to data dictionary
            self.data_dict[key][&#39;data&#39;][&#39;p_corrected&#39;] = FA._backg_correct(p_df, &#39;p_corrected&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;s_corrected&#39;] = FA._backg_correct(s_df, &#39;s_corrected&#39;)
            
            print(&#39;Background correction has been successfully performed!&#39;)
            
    def _backg_correct(df, col_name):
        &#34;&#34;&#34;Calculate background corrected p or s.
        
        :param df: Joined platemap and raw p or s values 
        :type df: pandas df
        :param col_name: Name of the column with background corrected values
        :type col_name: str
        :return: Data frame with background corrected p or s values (depending on col_name parameter)
        :rtype: pandas df&#34;&#34;&#34;
        
        df[df.columns[-1]] = df[df.columns[-1]][df[&#39;Valid&#39;] == True]   # &#39;p&#39; or &#39;s&#39; values are replaced with NaN if the well is invalidated
        no_index = df.reset_index()   # move the index to df column

        mindex = pd.MultiIndex.from_frame(no_index[[&#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;]])   # create multiindex
        reindexed = no_index.set_index(mindex).drop([&#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;], axis=1)   # add multiindex to df and drop the columns from which multiindex was created
        
        mean = reindexed.groupby(level=[0,1,2]).mean().drop(&#39;Valid&#39;, axis=1)   # calculate mean for each group of three wells and remove &#39;Valid&#39; column
        mean.rename(columns={mean.columns[-1]: &#39;Mean&#39;}, inplace=True)   # rename the last column to &#39;Mean&#39;

        blank = mean.xs(&#39;blank&#39;, level=0, drop_level=True)   # take a group with blank wells
        
        joined = reindexed.join(blank, on=[&#39;Protein Name&#39;, &#39;Protein Concentration&#39;])
        joined[col_name] = joined[joined.columns[-2]] - joined[&#39;Mean&#39;]   # calculate background corrected values
        jindexed = joined.set_index(&#39;index&#39;, append=True).reset_index(level=[0,1,2]).rename_axis(None)   # set index to &#39;well id&#39; and move multiindex to df columns
        
        return jindexed[[col_name]]
    
    
    def calculate_r_i(self, corrected=True, th=80):
        &#34;&#34;&#34;Calculates anisotropy and fluorescence intensity.
        The fluorescence intensity (I) and anisotropy (r) are calculated using the follwing formulas: I = s + (2*g*p), r = (s - (g*p)) / I and stored 
        in data_dict as i_raw and r_raw (calculated using the uncorrected p and s channel values) 
        and i_corrected and r_corrected (if calculated using the background corrected p and s channel values, as well).
        
        :param corrected: Calculate the anisotropy and intensity using the background corrected values of p and s, as well, default=True
        :type corrected: bool&#34;&#34;&#34;
        d = []
        
        for key, value in self.data_dict.items():   # iterate over all repeats
            metadata, data = value.values()
            
            # calculate raw intensity and anisotropy and add them to data dictionary
            i, r = FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;i_raw&#39;] = i   #FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;, &#39;i&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;r_raw&#39;] = r   #FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;, &#39;r&#39;)
            
            if corrected:   # calculate intensity and anisotropy using background corrected values of p and s
                if &#39;p_corrected&#39; and &#39;s_corrected&#39; not in data:   # check if background subtraction has been done
                    raise AttributeError(&#39;The corrected anisotropy and intensity can only be calculated after background correction of the raw p and s channel data.&#39;)
                
                i_c, r_c = FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;)
                self.data_dict[key][&#39;data&#39;][&#39;i_corrected&#39;] = i_c   #FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;, &#39;i&#39;)
                self.data_dict[key][&#39;data&#39;][&#39;r_corrected&#39;] = r_c   #FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;, &#39;r&#39;) 
                
                d.append(FA._calc_I_percent(key, th, i, i_c, self.plate_map))
                
        print(&#39;Fluorescence intensity and anisotropy have been calculated!\n&#39;)
        print(d)

        
    def _calc_r_I(p, s, g, col_suffix):
        &#34;&#34;&#34;Calculates either anisotropy or intensity and labels the resulting dfs according to the parameters passed
        
        :param p: Pandas data frame with p channel data (can be both raw and background corrected)
        :type p: pandas df 
        :param s: Pandas data frame with s channel data (can be both raw and background corrected)
        :type s: pandas df
        :param col_suffix: Suffix to add to column name of the resulting intensity or anisotropy data frame, e.g. &#39;raw&#39;, &#39;corrected&#39;
        :type col_suffix: str
        :return: Data frame with calculated anisotropy or intensity values
        :rtype: pandas df&#34;&#34;&#34;
        
        p_rn = p.rename(columns={p.columns[0]: s.columns[0]})   # rename the col name in p data frame so that both p and s dfs have the same col names to enable calculation on dfs
        i = s + (2 * g * p_rn)   # calculate intensity irrespectively of the var arguent    
        r = (s - (g * p_rn)) / i
        i_rn = i.rename(columns={i.columns[0]: &#39;i_&#39;+col_suffix})
        r_rn = r.rename(columns={r.columns[0]: &#39;r_&#39;+col_suffix})   # rename the col name        
            
        return i_rn, r_rn  
    
    
    def _calc_I_percent(rep, th, ir, ic, platemap):
        ir_rn = ir.rename(columns={ir.columns[0]:ic.columns[0]})
        ratio = (ir_rn - ic)/ir_rn * 100
        ratio.rename(columns={&#39;i_corrected&#39;:&#39;percent&#39;}, inplace=True)
        j = platemap.join(ratio)
        subset = j[(j[&#39;Type&#39;] != &#39;blank&#39;) &amp; (j[&#39;Type&#39;] != &#39;empty&#39;)]
        wells = list(subset[subset[&#39;percent&#39;] &gt; th].index)
        return rep, wells, subset</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="flu_ani_analysis.flu_ani_analysis_module.FA.read_in_envision"><code class="name flex">
<span>def <span class="ident">read_in_envision</span></span>(<span>data_csv, platemap_csv, data_type='plate', size=384)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a dictionary of data frames, g-factor and platemap needed to construct the class object. </p>
<p>:param data_csv: File path of the raw data file in .csv format
:type data_csv: str
:param platemap_csv: File path of the platemap file in .csv format
:type platemap_csv: str
:param data_type: Format in which the raw data was exported (plate or list), defaults to plate
:type data_type: str
:param size: Size of the well plate (384 or 96), defaults to 384
:type size: int
:return: A dictionary contaning data frames with pre-processed data, g-factor, pandas data frame containing platemap
:rtype: dict, float, pandas data frame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def read_in_envision(cls, data_csv, platemap_csv, data_type=&#39;plate&#39;, size=384):
    &#34;&#34;&#34;Returns a dictionary of data frames, g-factor and platemap needed to construct the class object. 
    
    :param data_csv: File path of the raw data file in .csv format
    :type data_csv: str
    :param platemap_csv: File path of the platemap file in .csv format
    :type platemap_csv: str
    :param data_type: Format in which the raw data was exported (plate or list), defaults to plate
    :type data_type: str
    :param size: Size of the well plate (384 or 96), defaults to 384
    :type size: int
    :return: A dictionary contaning data frames with pre-processed data, g-factor, pandas data frame containing platemap
    :rtype: dict, float, pandas data frame &#34;&#34;&#34;
    
    # ensure the plate size is either 384 or 96
    if size not in plate_dim:
        raise PlateSizeError(&#39;Invalid size of the well plate, should be 384 or 96.&#39;)
    
    # try to read in data in plate format
    if data_type == &#39;plate&#39;:
        try:
            data_dict, g_factor = FA._read_in_plate(data_csv, size=size)
            plate_map_df = pm.plate_map(platemap_csv, size=size)
            return cls(data_dict, g_factor, plate_map_df)
        
        except (UnboundLocalError, IndexError, ValueError):
            raise DataError(f&#34;Error occured during data read in. Check your file contains data in the &#39;plate&#39; format and plate size is {size}.&#34;)
    
    # try to read in data in list format
    if data_type == &#39;list&#39;:
        try:
            data_dict, g_factor = FA._read_in_list(data_csv, size=size)
            plate_map_df = pm.plate_map(platemap_csv, size=size)
            return cls(data_dict, g_factor, plate_map_df)
        
        except (UnboundLocalError, IndexError):
            raise DataError(&#34;Error occured during data read in. Check your file contains data in the &#39;list&#39; format.&#34;)
    
    else:
        raise DataTypeError(f&#34;&#39;{data_type}&#39; is not one of the two valid data types: plate or list.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flu_ani_analysis.flu_ani_analysis_module.FA.background_correct"><code class="name flex">
<span>def <span class="ident">background_correct</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate background corrected values for p and s channel in all repeats.</p>
<p>Cacluclated by subtracting the mean value of blank p or s for a given concentration from each value of compound p or s for that concentration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def background_correct(self):
    &#34;&#34;&#34;Calculate background corrected values for p and s channel in all repeats.
    
    Cacluclated by subtracting the mean value of blank p or s for a given concentration from each value of compound p or s for that concentration.&#34;&#34;&#34;
    
    for key, value in self.data_dict.items(): 
        metadata, data = value.values()   
        p, s = data.values()             
        
        # create joined dfs of platemap and p or s
        p_df = self.plate_map.join(p)  
        s_df = self.plate_map.join(s)
        
        # calculate p and s corrected and add them to data dictionary
        self.data_dict[key][&#39;data&#39;][&#39;p_corrected&#39;] = FA._backg_correct(p_df, &#39;p_corrected&#39;)
        self.data_dict[key][&#39;data&#39;][&#39;s_corrected&#39;] = FA._backg_correct(s_df, &#39;s_corrected&#39;)
        
        print(&#39;Background correction has been successfully performed!&#39;)</code></pre>
</details>
</dd>
<dt id="flu_ani_analysis.flu_ani_analysis_module.FA.calculate_r_i"><code class="name flex">
<span>def <span class="ident">calculate_r_i</span></span>(<span>self, corrected=True, th=80)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates anisotropy and fluorescence intensity.
The fluorescence intensity (I) and anisotropy (r) are calculated using the follwing formulas: I = s + (2<em>g</em>p), r = (s - (g*p)) / I and stored
in data_dict as i_raw and r_raw (calculated using the uncorrected p and s channel values)
and i_corrected and r_corrected (if calculated using the background corrected p and s channel values, as well).</p>
<p>:param corrected: Calculate the anisotropy and intensity using the background corrected values of p and s, as well, default=True
:type corrected: bool</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_r_i(self, corrected=True, th=80):
    &#34;&#34;&#34;Calculates anisotropy and fluorescence intensity.
    The fluorescence intensity (I) and anisotropy (r) are calculated using the follwing formulas: I = s + (2*g*p), r = (s - (g*p)) / I and stored 
    in data_dict as i_raw and r_raw (calculated using the uncorrected p and s channel values) 
    and i_corrected and r_corrected (if calculated using the background corrected p and s channel values, as well).
    
    :param corrected: Calculate the anisotropy and intensity using the background corrected values of p and s, as well, default=True
    :type corrected: bool&#34;&#34;&#34;
    d = []
    
    for key, value in self.data_dict.items():   # iterate over all repeats
        metadata, data = value.values()
        
        # calculate raw intensity and anisotropy and add them to data dictionary
        i, r = FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;)
        self.data_dict[key][&#39;data&#39;][&#39;i_raw&#39;] = i   #FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;, &#39;i&#39;)
        self.data_dict[key][&#39;data&#39;][&#39;r_raw&#39;] = r   #FA._calc_r_I(data[&#39;p&#39;], data[&#39;s&#39;], self.g_factor, &#39;raw&#39;, &#39;r&#39;)
        
        if corrected:   # calculate intensity and anisotropy using background corrected values of p and s
            if &#39;p_corrected&#39; and &#39;s_corrected&#39; not in data:   # check if background subtraction has been done
                raise AttributeError(&#39;The corrected anisotropy and intensity can only be calculated after background correction of the raw p and s channel data.&#39;)
            
            i_c, r_c = FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;i_corrected&#39;] = i_c   #FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;, &#39;i&#39;)
            self.data_dict[key][&#39;data&#39;][&#39;r_corrected&#39;] = r_c   #FA._calc_r_I(data[&#39;p_corrected&#39;], data[&#39;s_corrected&#39;], self.g_factor, &#39;corrected&#39;, &#39;r&#39;) 
            
            d.append(FA._calc_I_percent(key, th, i, i_c, self.plate_map))
            
    print(&#39;Fluorescence intensity and anisotropy have been calculated!\n&#39;)
    print(d)</code></pre>
</details>
</dd>
<dt id="flu_ani_analysis.flu_ani_analysis_module.FA.invalidate"><code class="name flex">
<span>def <span class="ident">invalidate</span></span>(<span>self, valid=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Invalidates wells, columns and/or rows. Any of the following arguments, or their combination, can be passed: wells, rows, columns.
For example to invalidate well A1, rows C and D and columns 7 and 8 execute the following: invalidate(wells='A1', rows=['C','D'], columns=[7,8]).
To validate previously invalidated wells, rows and/or columns, pass the additional 'valid' argument as True.</p>
<p>:param valid: Sets the stipulated row or rows 'True' or 'False', default = False
:type valid: bool
:param wells: Wells to be invalidated passed as a string or a list of strings
:type wells: str or list
:param rows: Rows to be invalidated passed as a string or a list of strings
:type rows: str or list
:param columns: Columns to be invalidated passed as an integer or a list of integers
:type columns: int or list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def invalidate(self, valid=False, **kwargs):
    &#34;&#34;&#34;Invalidates wells, columns and/or rows. Any of the following arguments, or their combination, can be passed: wells, rows, columns. 
    For example to invalidate well A1, rows C and D and columns 7 and 8 execute the following: invalidate(wells=&#39;A1&#39;, rows=[&#39;C&#39;,&#39;D&#39;], columns=[7,8]).
    To validate previously invalidated wells, rows and/or columns, pass the additional &#39;valid&#39; argument as True.

    :param valid: Sets the stipulated row or rows &#39;True&#39; or &#39;False&#39;, default = False
    :type valid: bool
    :param wells: Wells to be invalidated passed as a string or a list of strings
    :type wells: str or list
    :param rows: Rows to be invalidated passed as a string or a list of strings
    :type rows: str or list
    :param columns: Columns to be invalidated passed as an integer or a list of integers
    :type columns: int or list
    &#34;&#34;&#34;
    # execute the corresponding invalidate functon from the platemapping package
    if &#39;wells&#39; in kwargs:
        pm.invalidate_wells(platemap=self.plate_map, wells=kwargs[&#39;wells&#39;], valid=valid)
    if &#39;rows&#39; in kwargs:
        rows = tuple(kwargs[&#39;rows&#39;]) # convert the rows to tuple because invalidate_rows cannot take in a list
        pm.invalidate_rows(platemap=self.plate_map, rows=rows, valid=valid)
    if &#39;columns&#39; in kwargs:
        pm.invalidate_cols(platemap=self.plate_map, cols=kwargs[&#39;columns&#39;], valid=valid)
    if len(kwargs) == 0:   # return error if neither of the keyword arguments is passed
        raise TypeError(&#39;No arguments were passed. Specify the wells, rows or columns to be invalidated!&#39;)</code></pre>
</details>
</dd>
<dt id="flu_ani_analysis.flu_ani_analysis_module.FA.visualise"><code class="name flex">
<span>def <span class="ident">visualise</span></span>(<span>self, colorby='Type', labelby='Type', title='', cmap='Paired', dpi=250, export=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a visual representation of the plate map.
The label and colour for each well can be customised to be a variable, for example 'Type', 'Protein Name', 'Protein Concentration', etc.
It can also be the p or s anisotropy value from a specified repeat passed as a tuple of strings, for example ('repeat_2', 'p') for p data from repeat 2</p>
<p>:param colorby: Chooses the parameter to color code by, for example 'Type', 'Contents', 'Protein Concentration', ('repeat_2', 'p'), default = 'Type'
:type colorby: str or tuple
:param labelby: Chooses the parameter to label code by, for example 'Type', 'Contents', 'Protein', ('repeat_1', 's'), default = 'Type'
:type labelby: str or tuple
:param title: Sets the title of the figure, default none
:type title: str
:param cmap: Sets the colormap for the color-coding, default = 'Paired'
:type cmap: str
:param dpi: Size of the figure, default = 250
:type dpi: int
:param export: If 'True' a .png file of the figure is saved, default = False
:type export: bool
:return: Visual representation of the plate map.
:rtype: figure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualise(self, colorby=&#39;Type&#39;, labelby=&#39;Type&#39;, title=&#34;&#34;, cmap=&#39;Paired&#39;, dpi=250, export=False):
    &#34;&#34;&#34;Returns a visual representation of the plate map.
    The label and colour for each well can be customised to be a variable, for example &#39;Type&#39;, &#39;Protein Name&#39;, &#39;Protein Concentration&#39;, etc.
    It can also be the p or s anisotropy value from a specified repeat passed as a tuple of strings, for example (&#39;repeat_2&#39;, &#39;p&#39;) for p data from repeat 2
    
    :param colorby: Chooses the parameter to color code by, for example &#39;Type&#39;, &#39;Contents&#39;, &#39;Protein Concentration&#39;, (&#39;repeat_2&#39;, &#39;p&#39;), default = &#39;Type&#39;
    :type colorby: str or tuple
    :param labelby: Chooses the parameter to label code by, for example &#39;Type&#39;, &#39;Contents&#39;, &#39;Protein&#39;, (&#39;repeat_1&#39;, &#39;s&#39;), default = &#39;Type&#39;
    :type labelby: str or tuple
    :param title: Sets the title of the figure, default none
    :type title: str
    :param cmap: Sets the colormap for the color-coding, default = &#39;Paired&#39;
    :type cmap: str
    :param dpi: Size of the figure, default = 250
    :type dpi: int
    :param export: If &#39;True&#39; a .png file of the figure is saved, default = False
    :type export: bool
    :return: Visual representation of the plate map.
    :rtype: figure
    &#34;&#34;&#34;
    plate_map = self.plate_map
    scinot = False
    
    if type(labelby) == tuple:   # option for labelling by the p or s anisotropy values
        plate_map = self.plate_map.join(self.data_dict[labelby[0]][&#39;data&#39;][labelby[1]])   # data frame containing p or s values from specified repeat is added to the platemap
        labelby = labelby[1]
    if type(colorby) == tuple:   # option for colouring by the p or s anisotropy values
        plate_map = self.plate_map.join(self.data_dict[colorby[0]][&#39;data&#39;][colorby[1]])
        colorby = colorby[1]
        
    size = plate_map.shape[0]
    
    if labelby in [&#39;Protein Concentration&#39;, &#39;Tracer Concentration&#39;, &#39;Competitor Concentration&#39;, &#39;p&#39;, &#39;s&#39;, &#39;p_corrected&#39;, &#39;s_corrected&#39;, &#39;r_raw&#39;, &#39;r_corrected&#39;, &#39;i_raw&#39; , &#39;i_corrected&#39;]:
        if sum((plate_map[labelby] &gt; 1000) | (plate_map[labelby] &lt; 0)) &gt; 0:   # display in sci notation if the number is greater than 1000 or less than 0
            scinot = True
    
    return pm.visualise(platemap=plate_map, title=title, size=size, export=export, cmap=cmap, colorby=colorby, labelby=labelby, scinot=scinot, dpi=dpi)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flu_ani_analysis.flu_ani_analysis_module.PlateSizeError"><code class="flex name class">
<span>class <span class="ident">PlateSizeError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PlateSizeError(Exception):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flu_ani_analysis" href="index.html">flu_ani_analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flu_ani_analysis.flu_ani_analysis_module.DataError" href="#flu_ani_analysis.flu_ani_analysis_module.DataError">DataError</a></code></h4>
</li>
<li>
<h4><code><a title="flu_ani_analysis.flu_ani_analysis_module.DataTypeError" href="#flu_ani_analysis.flu_ani_analysis_module.DataTypeError">DataTypeError</a></code></h4>
</li>
<li>
<h4><code><a title="flu_ani_analysis.flu_ani_analysis_module.FA" href="#flu_ani_analysis.flu_ani_analysis_module.FA">FA</a></code></h4>
<ul class="">
<li><code><a title="flu_ani_analysis.flu_ani_analysis_module.FA.background_correct" href="#flu_ani_analysis.flu_ani_analysis_module.FA.background_correct">background_correct</a></code></li>
<li><code><a title="flu_ani_analysis.flu_ani_analysis_module.FA.calculate_r_i" href="#flu_ani_analysis.flu_ani_analysis_module.FA.calculate_r_i">calculate_r_i</a></code></li>
<li><code><a title="flu_ani_analysis.flu_ani_analysis_module.FA.invalidate" href="#flu_ani_analysis.flu_ani_analysis_module.FA.invalidate">invalidate</a></code></li>
<li><code><a title="flu_ani_analysis.flu_ani_analysis_module.FA.read_in_envision" href="#flu_ani_analysis.flu_ani_analysis_module.FA.read_in_envision">read_in_envision</a></code></li>
<li><code><a title="flu_ani_analysis.flu_ani_analysis_module.FA.visualise" href="#flu_ani_analysis.flu_ani_analysis_module.FA.visualise">visualise</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flu_ani_analysis.flu_ani_analysis_module.PlateSizeError" href="#flu_ani_analysis.flu_ani_analysis_module.PlateSizeError">PlateSizeError</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>